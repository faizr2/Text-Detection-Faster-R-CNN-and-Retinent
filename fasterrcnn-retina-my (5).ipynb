{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2026-02-06T12:51:53.014805Z",
     "iopub.status.busy": "2026-02-06T12:51:53.014568Z",
     "iopub.status.idle": "2026-02-06T12:51:54.198851Z",
     "shell.execute_reply": "2026-02-06T12:51:54.198034Z",
     "shell.execute_reply.started": "2026-02-06T12:51:53.014781Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to dataset files: /kaggle/input/icdar2015\n"
     ]
    }
   ],
   "source": [
    "# import kagglehub\n",
    "\n",
    "# # Download latest version\n",
    "# path = kagglehub.dataset_download(\"bestofbests9/icdar2015\")\n",
    "\n",
    "# print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-08T10:58:32.271114Z",
     "iopub.status.busy": "2026-02-08T10:58:32.270541Z",
     "iopub.status.idle": "2026-02-08T10:58:32.421384Z",
     "shell.execute_reply": "2026-02-08T10:58:32.420920Z",
     "shell.execute_reply.started": "2026-02-08T10:58:32.271091Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# ИМПОРТЫ И НАСТРОЙКА ПЛАТФОРМЫ\n",
    "# ============================================================================\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import glob\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from typing import Dict, List, Tuple, Optional, Any, Union\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import precision_recall_curve, average_precision_score\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "from torchvision import transforms as T\n",
    "from torchvision.models.detection import FasterRCNN\n",
    "from torchvision.models.detection.retinanet import RetinaNet\n",
    "from torchvision.models.detection.backbone_utils import resnet_fpn_backbone\n",
    "from torchvision.models.detection.rpn import AnchorGenerator\n",
    "from torchvision.ops import MultiScaleRoIAlign, box_iou, nms\n",
    "\n",
    "import math\n",
    "from functools import partial\n",
    "from collections import Counter, defaultdict\n",
    "from torchvision.models.detection.anchor_utils import AnchorGenerator\n",
    "from torchmetrics.detection import MeanAveragePrecision\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ============================================================================\n",
    "# КОНФИГУРАЦИЯ\n",
    "# ============================================================================\n",
    "\n",
    "class Config:\n",
    "    \"\"\"Единый класс для управления конфигурацией\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.platform = self._detect_platform()\n",
    "        self.setup_paths()\n",
    "        self.setup_hyperparameters()\n",
    "        self.setup_training_params()\n",
    "        self.setup_myArch_params()\n",
    "\n",
    "    def setup_myArch_params(self):\n",
    "        \"\"\"Параметры для myArch модели\"\"\"\n",
    "        self.MYARCH_BACKBONE = 'resnet50' # 'efficientnet_b0'\n",
    "        self.MYARCH_INPUT_SIZE = (640, 640)\n",
    "        self.MYARCH_NECK_CHANNELS = 256\n",
    "        # Якоря для текста - маленькие и вытянутые!\n",
    "        self.MYARCH_ANCHOR_SIZES = (16, 32, 64, 128, 256)\n",
    "        self.MYARCH_ANCHOR_RATIOS = (0.1, 0.5, 1.0, 2.0, 10.0)\n",
    "        self.MYARCH_SCORE_THRESH = 0.05\n",
    "        self.MYARCH_NMS_THRESH = 0.3\n",
    "            \n",
    "    def _detect_platform(self) -> str:\n",
    "        \"\"\"Определяет платформу запуска\"\"\"\n",
    "        if 'google.colab' in str(sys.modules):\n",
    "            return 'colab'\n",
    "        elif 'KAGGLE_KERNEL_RUN_TYPE' in os.environ:\n",
    "            return 'kaggle'\n",
    "        return 'local'\n",
    "    \n",
    "    def setup_paths(self):\n",
    "        \"\"\"Настраивает пути в зависимости от платформы\"\"\"\n",
    "        if self.platform == 'kaggle':\n",
    "            self.BASE_DATA_DIR = '/kaggle/input'\n",
    "            self.BASE_WORKING_DIR = '/kaggle/working'\n",
    "            self.ICDAR2015_ROOT = Path(self.BASE_DATA_DIR) / 'icdar2015'\n",
    "            self.ICDAR2015_PROCESSED = Path(self.BASE_WORKING_DIR) / 'icdar2015_processed'\n",
    "            \n",
    "        elif self.platform == 'colab':\n",
    "            self.BASE_DATA_DIR = '/content/drive/MyDrive'\n",
    "            self.BASE_WORKING_DIR = '/content'\n",
    "            self.ICDAR2015_ROOT = Path(self.BASE_DATA_DIR) / 'icdar2015'\n",
    "            self.ICDAR2015_PROCESSED = Path(self.BASE_DATA_DIR) / 'icdar2015_processed'\n",
    "            \n",
    "        else:  # local\n",
    "            self.BASE_DATA_DIR = './data'\n",
    "            self.BASE_WORKING_DIR = './output'\n",
    "            self.ICDAR2015_ROOT = Path(self.BASE_DATA_DIR) / 'icdar2015'\n",
    "            self.ICDAR2015_PROCESSED = Path(self.BASE_WORKING_DIR) / 'icdar2015_processed'\n",
    "    \n",
    "    def setup_hyperparameters(self):\n",
    "        \"\"\"Настройки гиперпараметров\"\"\"\n",
    "        self.NUM_CLASSES = 2  # фон + текст\n",
    "        self.NUM_EPOCHS = 20\n",
    "        self.BATCH_SIZE = 4\n",
    "        self.LEARNING_RATE = 0.001\n",
    "        self.WEIGHT_DECAY = 0.0001\n",
    "        self.PATIENCE = 6   # ждать дольше нет смысла\n",
    "        self.MIN_SIZE = 600\n",
    "        self.MAX_SIZE = 1000\n",
    "        self.CONFIDENCE_THRESHOLD = 0.5\n",
    "        self.NMS_THRESHOLD = 0.3\n",
    "        \n",
    "    def setup_training_params(self):\n",
    "        # \"\"\"Параметры обучения\"\"\"\n",
    "        self.ARCHITECTURE = 'faster_rcnn' # 'retinanet' # 'faster_rcnn' # 'myArch'  \n",
    "        self.BACKBONE = 'resnet50'  # 'resnet50'\n",
    "        self.MODE = 'train' # 'visualize' # 'compare' # 'train'  # train, compare, visualize\n",
    "        self.MODEL_PATH = None  # путь для тестирования\n",
    "        self.COMPARE_ARCHITECTURES = ['retinanet', 'faster_rcnn']\n",
    "        self.VISUALIZE_SAMPLES = 5\n",
    "                \n",
    "    def get_train_paths(self) -> Dict[str, Path]:\n",
    "        \"\"\"Возвращает пути к тренировочным данным\"\"\"\n",
    "        return {\n",
    "            'images': self.ICDAR2015_ROOT / 'ch4_training_images',\n",
    "            'labels': self.ICDAR2015_ROOT / 'ch4_training_localization_transcription_gt'\n",
    "        }\n",
    "    \n",
    "    def get_test_paths(self) -> Dict[str, Path]:\n",
    "        \"\"\"Возвращает пути к тестовым данным\"\"\"\n",
    "        return {\n",
    "            'images': self.ICDAR2015_ROOT / 'ch4_test_images',\n",
    "            'labels': self.ICDAR2015_ROOT / 'ch4_test_localization_transcription_gt'\n",
    "        }\n",
    "    \n",
    "    def get_processed_paths(self) -> Dict[str, Path]:\n",
    "        \"\"\"Возвращает пути к обработанным данным\"\"\"\n",
    "        return {\n",
    "            'train_annotations': self.ICDAR2015_PROCESSED / 'train_annotations.json',\n",
    "            'val_annotations': self.ICDAR2015_PROCESSED / 'val_annotations.json',\n",
    "            'config': self.ICDAR2015_PROCESSED / 'config.json'\n",
    "        }\n",
    "    \n",
    "    def check_data_structure(self) -> bool:\n",
    "        \"\"\"Проверяет структуру данных\"\"\"\n",
    "        paths = [\n",
    "            ('Тренировочные изображения', self.get_train_paths()['images']),\n",
    "            ('Тренировочные аннотации', self.get_train_paths()['labels']),\n",
    "            ('Тестовые изображения', self.get_test_paths()['images']),\n",
    "            ('Тестовые аннотации', self.get_test_paths()['labels'])\n",
    "        ]\n",
    "        \n",
    "        all_exist = True\n",
    "        for name, path in paths:\n",
    "            if path.exists():\n",
    "                jpg_files = list(path.glob('*.jpg'))\n",
    "                txt_files = list(path.glob('*.txt'))\n",
    "                png_files = list(path.glob('*.png'))\n",
    "                count = len(jpg_files) + len(txt_files) + len(png_files)\n",
    "                print(f\" {name}: {count} файлов\")\n",
    "            else:\n",
    "                print(f\" {name}: не найдено\")\n",
    "                all_exist = False\n",
    "        return all_exist\n",
    "    \n",
    "    def print_info(self):\n",
    "        \"\"\"Выводит информацию о конфигурации\"\"\"\n",
    "        print(\"=\"*60)\n",
    "        print(\"КОНФИГУРАЦИЯ СИСТЕМЫ\")\n",
    "        print(\"=\"*60)\n",
    "        print(f\"Платформа: {self.platform.upper()}\")\n",
    "        print(f\"Режим: {self.MODE.upper()}\")\n",
    "        print(f\"Архитектура: {self.ARCHITECTURE}\")\n",
    "        print(f\"Данные: {self.ICDAR2015_ROOT}\")\n",
    "        print(f\"Устройство: {'CUDA' if torch.cuda.is_available() else 'CPU'}\")\n",
    "        print(\"=\"*60)\n",
    "\n",
    "# ============================================================================\n",
    "# ОБРАБОТКА ДАННЫХ\n",
    "# ============================================================================\n",
    "\n",
    "class ICDAR2015ToCOCOConverter:\n",
    "    \"\"\"Конвертер из формата ICDAR2015 в COCO\"\"\"\n",
    "    \n",
    "    def __init__(self, images_dir: Path, labels_dir: Path, class_names: List[str]):\n",
    "        self.images_dir = images_dir\n",
    "        self.labels_dir = labels_dir\n",
    "        self.class_names = class_names\n",
    "        self.coco_data = {\n",
    "            \"images\": [],\n",
    "            \"annotations\": [],\n",
    "            \"categories\": []\n",
    "        }\n",
    "        self.annotation_id = 0\n",
    "        \n",
    "        # Создаем категории\n",
    "        for i, name in enumerate(self.class_names):\n",
    "            self.coco_data[\"categories\"].append({\n",
    "                \"id\": i,\n",
    "                \"name\": name,\n",
    "                \"supercategory\": \"object\"\n",
    "            })\n",
    "    \n",
    "    def convert(self, output_path: Path) -> Dict[str, List]:\n",
    "        \"\"\"Конвертирует ICDAR2015 в COCO формат\"\"\"\n",
    "        image_files = list(self.images_dir.glob(\"*.jpg\"))\n",
    "        if not image_files:\n",
    "            image_files = list(self.images_dir.glob(\"*.png\"))\n",
    "        \n",
    "        print(f\"Найдено {len(image_files)} изображений\")\n",
    "        \n",
    "        for img_id, img_path in enumerate(tqdm(image_files, desc=\"Конвертация ICDAR2015 → COCO\")):\n",
    "            try:\n",
    "                # Загрузка изображения\n",
    "                img = cv2.imread(str(img_path))\n",
    "                if img is None:\n",
    "                    print(f\"Не удалось загрузить: {img_path}\")\n",
    "                    continue\n",
    "                    \n",
    "                height, width = img.shape[:2]\n",
    "                \n",
    "                # Информация об изображении\n",
    "                image_info = {\n",
    "                    \"id\": img_id,\n",
    "                    \"file_name\": img_path.name,\n",
    "                    \"width\": width,\n",
    "                    \"height\": height\n",
    "                }\n",
    "                self.coco_data[\"images\"].append(image_info)\n",
    "                \n",
    "                # Обработка аннотаций\n",
    "                base_name = img_path.stem\n",
    "                anno_file = self.labels_dir / f\"gt_{base_name}.txt\"\n",
    "                \n",
    "                if anno_file.exists():\n",
    "                    self._process_annotation(anno_file, img_id)\n",
    "                else:\n",
    "                    # Попробуем другой формат имени файла\n",
    "                    alt_anno_file = self.labels_dir / f\"{base_name}.txt\"\n",
    "                    if alt_anno_file.exists():\n",
    "                        self._process_annotation(alt_anno_file, img_id)\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"Ошибка при обработке {img_path}: {e}\")\n",
    "        \n",
    "        # Сохранение\n",
    "        output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        with open(output_path, 'w') as f:\n",
    "            json.dump(self.coco_data, f, indent=2)\n",
    "        \n",
    "        print(f\"\\n COCO аннотации сохранены в {output_path}\")\n",
    "        print(f\"  Изображений: {len(self.coco_data['images'])}\")\n",
    "        print(f\"  Аннотаций: {len(self.coco_data['annotations'])}\")\n",
    "        \n",
    "        return self.coco_data\n",
    "    \n",
    "    def _process_annotation(self, anno_file: Path, image_id: int):\n",
    "        \"\"\"Обрабатывает файл аннотации ICDAR2015\"\"\"\n",
    "        try:\n",
    "            with open(anno_file, 'r', encoding='utf-8-sig') as f:\n",
    "                lines = f.readlines()\n",
    "        except UnicodeDecodeError:\n",
    "            try:\n",
    "                with open(anno_file, 'r', encoding='latin-1') as f:\n",
    "                    lines = f.readlines()\n",
    "            except:\n",
    "                with open(anno_file, 'r', encoding='iso-8859-1') as f:\n",
    "                    lines = f.readlines()\n",
    "        \n",
    "        for line in lines:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "                \n",
    "            try:\n",
    "                # ICDAR2015 формат: x1,y1,x2,y2,x3,y3,x4,y4,text\n",
    "                parts = line.split(',')\n",
    "                \n",
    "                # Пробуем разные варианты разделителей\n",
    "                if len(parts) < 8:\n",
    "                    parts = line.split('\\t')\n",
    "                if len(parts) < 8:\n",
    "                    parts = line.split(' ')\n",
    "                \n",
    "                if len(parts) < 8:\n",
    "                    continue\n",
    "                    \n",
    "                # Парсинг координат\n",
    "                coords = []\n",
    "                for part in parts[:8]:\n",
    "                    try:\n",
    "                        coords.append(int(float(part.strip())))\n",
    "                    except:\n",
    "                        coords.append(int(part.strip()))\n",
    "                \n",
    "                x_coords = coords[0::2]\n",
    "                y_coords = coords[1::2]\n",
    "                \n",
    "                # Создание bounding box\n",
    "                x_min, x_max = min(x_coords), max(x_coords)\n",
    "                y_min, y_max = min(y_coords), max(y_coords)\n",
    "                width, height = x_max - x_min, y_max - y_min\n",
    "                \n",
    "                # Проверка валидности bounding box\n",
    "                if width <= 2 or height <= 2 or width > 10000 or height > 10000:\n",
    "                    continue\n",
    "                \n",
    "                # Создание аннотации\n",
    "                annotation = {\n",
    "                    \"id\": self.annotation_id,\n",
    "                    \"image_id\": image_id,\n",
    "                    \"category_id\": 0,  # только один класс - текст\n",
    "                    \"bbox\": [float(x_min), float(y_min), float(width), float(height)],\n",
    "                    \"area\": float(width * height),\n",
    "                    \"iscrowd\": 0,\n",
    "                    \"segmentation\": [],\n",
    "                    \"text\": parts[8] if len(parts) > 8 else \"\"\n",
    "                }\n",
    "                \n",
    "                self.coco_data[\"annotations\"].append(annotation)\n",
    "                self.annotation_id += 1\n",
    "                \n",
    "            except (ValueError, IndexError) as e:\n",
    "                # Пропускаем некорректные строки\n",
    "                continue\n",
    "\n",
    "# ============================================================================\n",
    "# ДАТАСЕТ\n",
    "# ============================================================================\n",
    "\n",
    "class ICDAR2015Dataset(Dataset):\n",
    "    \"\"\"Датасет для данных ICDAR2015 в COCO формате\"\"\"\n",
    "    \n",
    "    def __init__(self, images_dir: Path, annotations_path: Path, \n",
    "                 transform: Optional[callable] = None, \n",
    "                 max_size: Optional[int] = None):\n",
    "        self.images_dir = images_dir\n",
    "        self.transform = transform\n",
    "        \n",
    "        with open(annotations_path, 'r') as f:\n",
    "            self.coco_data = json.load(f)\n",
    "        \n",
    "        # Создаем индексы для быстрого доступа\n",
    "        self._create_indices()\n",
    "        \n",
    "        # Ограничение размера датасета (для отладки)\n",
    "        if max_size and max_size < len(self.image_ids):\n",
    "            self.image_ids = self.image_ids[:max_size]\n",
    "    \n",
    "    def _create_indices(self):\n",
    "        \"\"\"Создает индексы для быстрого доступа\"\"\"\n",
    "        self.image_id_to_info = {img[\"id\"]: img for img in self.coco_data[\"images\"]}\n",
    "        self.image_id_to_annotations = {}\n",
    "        \n",
    "        for ann in self.coco_data[\"annotations\"]:\n",
    "            img_id = ann[\"image_id\"]\n",
    "            if img_id not in self.image_id_to_annotations:\n",
    "                self.image_id_to_annotations[img_id] = []\n",
    "            self.image_id_to_annotations[img_id].append(ann)\n",
    "        \n",
    "        self.image_ids = list(self.image_id_to_info.keys())\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.image_ids)\n",
    "    \n",
    "    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, Dict[str, torch.Tensor]]:\n",
    "        image_id = self.image_ids[idx]\n",
    "        image_info = self.image_id_to_info[image_id]\n",
    "        \n",
    "        # Загрузка изображения\n",
    "        img_path = self.images_dir / image_info[\"file_name\"]\n",
    "        if not img_path.exists():\n",
    "            # Попробуем найти файл с другим расширением\n",
    "            img_path = self.images_dir / f\"{image_info['file_name'].split('.')[0]}.jpg\"\n",
    "            if not img_path.exists():\n",
    "                img_path = self.images_dir / f\"{image_info['file_name'].split('.')[0]}.png\"\n",
    "        \n",
    "        try:\n",
    "            image = Image.open(img_path).convert(\"RGB\")\n",
    "        except:\n",
    "            # Если не удалось загрузить, создаем пустое изображение\n",
    "            image = Image.new(\"RGB\", (image_info[\"width\"], image_info[\"height\"]), (128, 128, 128))\n",
    "        \n",
    "        # Получение аннотаций\n",
    "        annotations = self.image_id_to_annotations.get(image_id, [])\n",
    "        \n",
    "        # Извлечение bounding boxes и labels\n",
    "        boxes, labels = [], []\n",
    "        for ann in annotations:\n",
    "            x_min, y_min, width, height = ann[\"bbox\"]\n",
    "            \n",
    "            # Проверка и коррекция координат\n",
    "            x_min = max(0, x_min)\n",
    "            y_min = max(0, y_min)\n",
    "            width = max(1, min(width, image_info[\"width\"] - x_min))\n",
    "            height = max(1, min(height, image_info[\"height\"] - y_min))\n",
    "            \n",
    "            # Проверка валидности bounding box\n",
    "            if width > 0 and height > 0 and width <= image_info[\"width\"] and height <= image_info[\"height\"]:\n",
    "                boxes.append([x_min, y_min, x_min + width, y_min + height])\n",
    "                labels.append(ann[\"category_id\"] + 1)  # +1 для фона\n",
    "        \n",
    "        if not boxes:  # Если нет аннотаций, создаем пустые тензоры\n",
    "            boxes = torch.zeros((0, 4), dtype=torch.float32)\n",
    "            labels = torch.zeros(0, dtype=torch.int64)\n",
    "        else:\n",
    "            boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "            labels = torch.as_tensor(labels, dtype=torch.int64)\n",
    "        \n",
    "        # Создание target словаря\n",
    "        target = {\n",
    "            \"boxes\": boxes,\n",
    "            \"labels\": labels,\n",
    "            \"image_id\": torch.tensor([image_id]),\n",
    "            \"area\": (boxes[:, 2] - boxes[:, 0]) * (boxes[:, 3] - boxes[:, 1]) if len(boxes) > 0 else torch.tensor([]),\n",
    "            \"iscrowd\": torch.zeros(len(boxes), dtype=torch.int64)\n",
    "        }\n",
    "        \n",
    "        if self.transform:\n",
    "            try:\n",
    "                image = self.transform(image)\n",
    "            except:\n",
    "                # Если трансформация не удалась, используем базовую\n",
    "                image = T.ToTensor()(image)\n",
    "        \n",
    "        return image, target\n",
    "    \n",
    "    def get_image_info(self, idx: int) -> Dict[str, Any]:\n",
    "        \"\"\"Возвращает информацию об изображении\"\"\"\n",
    "        image_id = self.image_ids[idx]\n",
    "        return self.image_id_to_info[image_id]\n",
    "\n",
    "# ============================================================================\n",
    "# АУГМЕНТАЦИЯ И ТРАНСФОРМАЦИИ\n",
    "# ============================================================================\n",
    "\n",
    "def get_train_transforms():\n",
    "    \"\"\"Трансформации для тренировочных данных\"\"\"\n",
    "    return T.Compose([\n",
    "        T.ToTensor(),\n",
    "        T.RandomHorizontalFlip(p=0.3),\n",
    "        T.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "        T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "def get_val_transforms():\n",
    "    \"\"\"Трансформации для валидационных данных\"\"\"\n",
    "    return T.Compose([\n",
    "        T.ToTensor(),\n",
    "        T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# СВОЯ АРХИТЕКТУРА (основа взята с семинара)\n",
    "# ============================================================================\n",
    "\n",
    "class MyArchDetector(nn.Module):\n",
    "#     \"\"\"Максимально простая \"рабочая\" (ха-ха) версия детектора. На поиске текста она так и не заработала\"\"\"\n",
    "    \n",
    "    def __init__(self, backbone_name=\"resnet18\", num_classes=2, \n",
    "                 input_size=(640, 640), neck_channels=512):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.num_classes = num_classes\n",
    "        self.input_size = input_size\n",
    "        self.grid_size = 20  # Размер сетки для якорей\n",
    "        \n",
    "        # Используем предобученный ResNet в качестве backbone\n",
    "        self.backbone = torchvision.models.resnet18(pretrained=True)\n",
    "        \n",
    "        # Удаляем последние слои (avgpool и fc)\n",
    "        self.backbone = nn.Sequential(*list(self.backbone.children())[:-2])\n",
    "        \n",
    "        # Замораживаем первые слои\n",
    "        for name, param in self.backbone.named_parameters():\n",
    "            if 'layer1' in name or 'layer2' in name:\n",
    "                param.requires_grad = False\n",
    "        \n",
    "        # Feature Pyramid Network (упрощенная)\n",
    "        self.fpn = nn.ModuleDict({\n",
    "            'c5': nn.Conv2d(512, neck_channels, 1),\n",
    "            'c4': nn.Conv2d(256, neck_channels, 1),\n",
    "            'c3': nn.Conv2d(128, neck_channels, 1),\n",
    "        })\n",
    "        \n",
    "        self.fpn_upsample = nn.Upsample(scale_factor=2, mode='nearest')\n",
    "        \n",
    "        # Голова для классификации и регрессии\n",
    "        self.cls_head = nn.Sequential(\n",
    "            nn.Conv2d(neck_channels, neck_channels, 3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(neck_channels, 9 * (num_classes + 1), 3, padding=1)\n",
    "        )\n",
    "        \n",
    "        self.reg_head = nn.Sequential(\n",
    "            nn.Conv2d(neck_channels, neck_channels, 3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(neck_channels, 9 * 4, 3, padding=1)\n",
    "        )\n",
    "        \n",
    "        # Создаем якоря (3 размера * 3 соотношения = 9 якорей на клетку)\n",
    "        self.anchor_sizes = [32, 64, 128]  # в пикселях\n",
    "        self.anchor_ratios = [0.5, 1.0, 2.0]\n",
    "        self.num_anchors = len(self.anchor_sizes) * len(self.anchor_ratios)\n",
    "        \n",
    "        # Генерируем якоря для сетки\n",
    "        self.anchors = self._generate_anchors()\n",
    "        \n",
    "        # Инициализация\n",
    "        self._initialize_weights()\n",
    "    \n",
    "    def _initialize_weights(self):\n",
    "        \"\"\"Инициализация весов\"\"\"\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.normal_(m.weight, std=0.01)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.zeros_(m.bias)\n",
    "    \n",
    "    def _generate_anchors(self):\n",
    "        \"\"\"Генерация якорей для сетки 20x20\"\"\"\n",
    "        anchors = []\n",
    "        stride = self.input_size[0] // self.grid_size\n",
    "        \n",
    "        for i in range(self.grid_size):\n",
    "            for j in range(self.grid_size):\n",
    "                center_x = (j + 0.5) * stride\n",
    "                center_y = (i + 0.5) * stride\n",
    "                \n",
    "                for size in self.anchor_sizes:\n",
    "                    for ratio in self.anchor_ratios:\n",
    "                        w = size * math.sqrt(ratio)\n",
    "                        h = size / math.sqrt(ratio)\n",
    "                        \n",
    "                        x1 = center_x - w / 2\n",
    "                        y1 = center_y - h / 2\n",
    "                        x2 = center_x + w / 2\n",
    "                        y2 = center_y + h / 2\n",
    "                        \n",
    "                        anchors.append([x1, y1, x2, y2])\n",
    "        \n",
    "        return torch.tensor(anchors)  # [grid_size*grid_size*9, 4]\n",
    "    \n",
    "    def forward(self, images, targets=None):\n",
    "        device = images[0].device\n",
    "        \n",
    "        if self.training and targets is not None:\n",
    "            return self._forward_train(images, targets, device)\n",
    "        else:\n",
    "            return self._forward_inference(images, device)\n",
    "    \n",
    "    def _forward_train(self, images, targets, device):\n",
    "        \"\"\"Тренировочный forward\"\"\"\n",
    "        losses = {\n",
    "            'loss_classifier': torch.tensor(0.0, device=device),\n",
    "            'loss_box_reg': torch.tensor(0.0, device=device),\n",
    "            'loss_objectness': torch.tensor(0.0, device=device),\n",
    "            'loss_rpn_box_reg': torch.tensor(0.0, device=device),\n",
    "        }\n",
    "        \n",
    "        cls_losses = []\n",
    "        reg_losses = []\n",
    "        \n",
    "        anchors = self.anchors.to(device)\n",
    "        \n",
    "        for i, image in enumerate(images):\n",
    "            img_tensor = image.unsqueeze(0)\n",
    "            \n",
    "            # Resize до фиксированного размера\n",
    "            img_tensor = F.interpolate(img_tensor, size=self.input_size, \n",
    "                                      mode='bilinear', align_corners=False)\n",
    "            \n",
    "            # Forward через backbone\n",
    "            features = self.backbone(img_tensor)  # [1, 512, 20, 20]\n",
    "            \n",
    "            # Упрощенный FPN (только один уровень)\n",
    "            neck_features = self.fpn['c5'](features)\n",
    "            \n",
    "            # Forward через heads\n",
    "            cls_logits = self.cls_head(neck_features)  # [1, 9*3, 20, 20]\n",
    "            reg_preds = self.reg_head(neck_features)   # [1, 9*4, 20, 20]\n",
    "            \n",
    "            # Reshape predictions\n",
    "            batch_size, _, grid_h, grid_w = cls_logits.shape\n",
    "            \n",
    "            cls_logits = cls_logits.permute(0, 2, 3, 1).reshape(\n",
    "                batch_size, grid_h * grid_w * self.num_anchors, self.num_classes + 1\n",
    "            )\n",
    "            \n",
    "            reg_preds = reg_preds.permute(0, 2, 3, 1).reshape(\n",
    "                batch_size, grid_h * grid_w * self.num_anchors, 4\n",
    "            )\n",
    "            \n",
    "            # Получаем целевые боксы\n",
    "            target_boxes = targets[i]['boxes']\n",
    "            target_labels = targets[i]['labels']\n",
    "            \n",
    "            # Если нет объектов, все якоря - фон\n",
    "            if len(target_boxes) == 0:\n",
    "                assigned_labels = torch.zeros(grid_h * grid_w * self.num_anchors, \n",
    "                                            device=device, dtype=torch.long)\n",
    "                assigned_boxes = torch.zeros((grid_h * grid_w * self.num_anchors, 4), \n",
    "                                           device=device)\n",
    "            else:\n",
    "                # Вычисляем IoU между якорями и целевыми боксами\n",
    "                ious = box_iou(anchors, target_boxes)\n",
    "                \n",
    "                # Находим лучший IoU для каждого якоря\n",
    "                best_ious, best_idx = ious.max(dim=1)\n",
    "                \n",
    "                # Назначаем метки: IoU > 0.5 - позитив, < 0.3 - негатив\n",
    "                assigned_labels = torch.zeros_like(best_ious, dtype=torch.long)\n",
    "                pos_mask = best_ious > 0.5\n",
    "                neg_mask = best_ious < 0.3\n",
    "                \n",
    "                assigned_labels[pos_mask] = target_labels[best_idx[pos_mask]]\n",
    "                assigned_labels[neg_mask] = 0  # фон\n",
    "                \n",
    "                # Для игнорируемых якорей (0.3 <= IoU <= 0.5) оставляем -1\n",
    "                ignore_mask = ~(pos_mask | neg_mask)\n",
    "                assigned_labels[ignore_mask] = -1\n",
    "                \n",
    "                # Вычисляем целевые смещения для позитивных якорей\n",
    "                assigned_boxes = torch.zeros((len(anchors), 4), device=device)\n",
    "                \n",
    "                if pos_mask.any():\n",
    "                    pos_anchors = anchors[pos_mask]\n",
    "                    pos_targets = target_boxes[best_idx[pos_mask]]\n",
    "                    \n",
    "                    # Вычисляем смещения\n",
    "                    anchor_centers = (pos_anchors[:, :2] + pos_anchors[:, 2:]) / 2\n",
    "                    anchor_sizes = pos_anchors[:, 2:] - pos_anchors[:, :2]\n",
    "                    \n",
    "                    target_centers = (pos_targets[:, :2] + pos_targets[:, 2:]) / 2\n",
    "                    target_sizes = pos_targets[:, 2:] - pos_targets[:, :2]\n",
    "                    \n",
    "                    # Добавляем epsilon для стабильности\n",
    "                    eps = 1e-8\n",
    "                    anchor_sizes = torch.clamp(anchor_sizes, min=eps)\n",
    "                    target_sizes = torch.clamp(target_sizes, min=eps)\n",
    "                    \n",
    "                    dxdy = (target_centers - anchor_centers) / anchor_sizes\n",
    "                    dwh = torch.log(target_sizes / anchor_sizes)\n",
    "                    \n",
    "                    assigned_boxes[pos_mask] = torch.cat([dxdy, dwh], dim=1)\n",
    "            \n",
    "            # Классификация (игнорируем якоря с label = -1)\n",
    "            valid_mask = assigned_labels != -1\n",
    "            if valid_mask.any():\n",
    "                cls_loss = F.cross_entropy(\n",
    "                    cls_logits[0][valid_mask],\n",
    "                    assigned_labels[valid_mask],\n",
    "                    reduction='mean'\n",
    "                )\n",
    "            else:\n",
    "                cls_loss = torch.tensor(0.0, device=device)\n",
    "            \n",
    "            # Регрессия (только для позитивных якорей)\n",
    "            pos_mask = assigned_labels > 0\n",
    "            if pos_mask.any():\n",
    "                reg_loss = F.smooth_l1_loss(\n",
    "                    reg_preds[0][pos_mask],\n",
    "                    assigned_boxes[pos_mask],\n",
    "                    reduction='mean'\n",
    "                )\n",
    "            else:\n",
    "                reg_loss = torch.tensor(0.0, device=device)\n",
    "            \n",
    "            cls_losses.append(cls_loss)\n",
    "            reg_losses.append(reg_loss)\n",
    "        \n",
    "        # Усредняем losses с весами\n",
    "        if cls_losses and reg_losses:\n",
    "            losses['loss_classifier'] = torch.stack(cls_losses).mean() * 1.0\n",
    "            losses['loss_box_reg'] = torch.stack(reg_losses).mean() * 10.0  # Больший вес для регрессии\n",
    "        \n",
    "        return losses\n",
    "    \n",
    "    def _forward_inference(self, images, device):\n",
    "        \"\"\"Инференс\"\"\"\n",
    "        predictions = []\n",
    "        anchors = self.anchors.to(device)\n",
    "        \n",
    "        for i, image in enumerate(images):\n",
    "            img_tensor = image.unsqueeze(0)\n",
    "            \n",
    "            # Resize\n",
    "            img_tensor = F.interpolate(img_tensor, size=self.input_size, \n",
    "                                      mode='bilinear', align_corners=False)\n",
    "            \n",
    "            # Forward\n",
    "            features = self.backbone(img_tensor)\n",
    "            neck_features = self.fpn['c5'](features)\n",
    "            \n",
    "            cls_logits = self.cls_head(neck_features)\n",
    "            reg_preds = self.reg_head(neck_features)\n",
    "            \n",
    "            # Размеры\n",
    "            batch_size, _, grid_h, grid_w = cls_logits.shape\n",
    "            \n",
    "            # Reshape predictions\n",
    "            cls_logits = cls_logits.permute(0, 2, 3, 1).reshape(\n",
    "                batch_size, grid_h * grid_w * self.num_anchors, self.num_classes + 1\n",
    "            )\n",
    "            reg_preds = reg_preds.permute(0, 2, 3, 1).reshape(\n",
    "                batch_size, grid_h * grid_w * self.num_anchors, 4\n",
    "            )\n",
    "            \n",
    "            # Декодируем боксы\n",
    "            boxes = self._decode_boxes(anchors, reg_preds[0])\n",
    "            \n",
    "            # Получаем вероятности классов\n",
    "            scores = F.softmax(cls_logits[0], dim=-1)\n",
    "            \n",
    "            # Фильтруем предсказания\n",
    "            final_boxes, final_labels, final_scores = [], [], []\n",
    "            \n",
    "            for cls_idx in range(1, self.num_classes + 1):  # Пропускаем фон\n",
    "                cls_scores = scores[:, cls_idx]\n",
    "                \n",
    "                # Фильтрация по порогу\n",
    "                score_mask = cls_scores > 0.1\n",
    "                if not score_mask.any():\n",
    "                    continue\n",
    "                \n",
    "                cls_boxes = boxes[score_mask]\n",
    "                cls_scores = cls_scores[score_mask]\n",
    "                \n",
    "                # NMS\n",
    "                keep = nms(cls_boxes, cls_scores, 0.5)\n",
    "                \n",
    "                if len(keep) > 0:\n",
    "                    final_boxes.append(cls_boxes[keep])\n",
    "                    final_labels.append(torch.full((len(keep),), cls_idx, \n",
    "                                                  device=device, dtype=torch.int64))\n",
    "                    final_scores.append(cls_scores[keep])\n",
    "            \n",
    "            if final_boxes:\n",
    "                pred_dict = {\n",
    "                    'boxes': torch.cat(final_boxes, dim=0),\n",
    "                    'labels': torch.cat(final_labels, dim=0),\n",
    "                    'scores': torch.cat(final_scores, dim=0)\n",
    "                }\n",
    "            else:\n",
    "                pred_dict = {\n",
    "                    'boxes': torch.zeros((0, 4), device=device),\n",
    "                    'labels': torch.zeros(0, dtype=torch.int64, device=device),\n",
    "                    'scores': torch.zeros(0, device=device)\n",
    "                }\n",
    "            \n",
    "            predictions.append(pred_dict)\n",
    "        \n",
    "        return predictions\n",
    "    \n",
    "    def _decode_boxes(self, anchors, reg_preds):\n",
    "        \"\"\"Декодирует предсказанные смещения в боксы\"\"\"\n",
    "        anchor_centers = (anchors[:, :2] + anchors[:, 2:]) / 2\n",
    "        anchor_sizes = anchors[:, 2:] - anchors[:, :2]\n",
    "        \n",
    "        # Ограничиваем предсказания для стабильности\n",
    "        reg_preds = torch.clamp(reg_preds, -4.0, 4.0)\n",
    "        \n",
    "        centers = anchor_centers + reg_preds[:, :2] * anchor_sizes\n",
    "        sizes = anchor_sizes * torch.exp(reg_preds[:, 2:])\n",
    "        \n",
    "        boxes = torch.cat([\n",
    "            centers - sizes / 2,\n",
    "            centers + sizes / 2\n",
    "        ], dim=1)\n",
    "        \n",
    "        return boxes\n",
    "\n",
    "# ============================================================================\n",
    "# ФАБРИКА МОДЕЛЕЙ\n",
    "# ============================================================================\n",
    "\n",
    "class ModelFactory:\n",
    "    \"\"\"Фабрика для создания различных архитектур детекторов\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def create_model(architecture: str, num_classes: int = 2, **kwargs) -> nn.Module:\n",
    "        \"\"\"\n",
    "        Создание модели детекции по имени архитектуры\n",
    "        \n",
    "        Args:\n",
    "            architecture: Имя архитектуры\n",
    "            num_classes: Количество классов\n",
    "            **kwargs: Дополнительные параметры\n",
    "        \n",
    "        Returns:\n",
    "            Модель PyTorch\n",
    "        \"\"\"\n",
    "        \n",
    "        if architecture == 'faster_rcnn':\n",
    "            return ModelFactory.create_faster_rcnn(num_classes, **kwargs)\n",
    "        \n",
    "        elif architecture == 'retinanet':\n",
    "            return ModelFactory.create_retinanet(num_classes, **kwargs)\n",
    "        \n",
    "        elif architecture == 'myArch':\n",
    "            return ModelFactory.create_myArch(num_classes, **kwargs)\n",
    "        \n",
    "        else:\n",
    "            raise ValueError(f\"Неизвестная архитектура: {architecture}\")\n",
    "    \n",
    "\n",
    "    @staticmethod\n",
    "    def create_faster_rcnn(num_classes: int = 2, **kwargs):\n",
    "        \"\"\"Создание Faster R-CNN модели\"\"\"\n",
    "        backbone_name = kwargs.get('backbone', 'resnet50')\n",
    "        \n",
    "        backbone = resnet_fpn_backbone(\n",
    "            backbone_name=backbone_name,\n",
    "            weights='DEFAULT',\n",
    "            trainable_layers=5\n",
    "        )\n",
    "        \n",
    "        # Anchor generator\n",
    "        anchor_sizes = kwargs.get('anchor_sizes', ((32,), (64,), (128,), (256,), (512,)))\n",
    "        aspect_ratios = kwargs.get('aspect_ratios', ((0.5, 1.0, 2.0),) * len(anchor_sizes))\n",
    "        \n",
    "        anchor_generator = AnchorGenerator(\n",
    "            sizes=anchor_sizes,\n",
    "            aspect_ratios=aspect_ratios\n",
    "        )\n",
    "        \n",
    "        # ROI Align\n",
    "        roi_pooler = MultiScaleRoIAlign(\n",
    "            featmap_names=['0', '1', '2', '3'],\n",
    "            output_size=7,\n",
    "            sampling_ratio=2\n",
    "        )\n",
    "        \n",
    "        # Создание модели\n",
    "        model = FasterRCNN(\n",
    "            backbone,\n",
    "            num_classes=num_classes,\n",
    "            rpn_anchor_generator=anchor_generator,\n",
    "            box_roi_pool=roi_pooler,\n",
    "            min_size=kwargs.get('min_size', 600),\n",
    "            max_size=kwargs.get('max_size', 1000),\n",
    "            box_score_thresh=0.05,\n",
    "            box_nms_thresh=0.3,\n",
    "            box_detections_per_img=100\n",
    "        )\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    \n",
    "    @staticmethod\n",
    "    def create_retinanet(num_classes: int = 2, **kwargs):\n",
    "        \"\"\"Создание RetinaNet модели\"\"\"\n",
    "        from torchvision.models.detection.anchor_utils import AnchorGenerator\n",
    "        \n",
    "        backbone_name = kwargs.get('backbone', 'resnet18')\n",
    "        \n",
    "        # Backbone\n",
    "        backbone = resnet_fpn_backbone(\n",
    "            backbone_name=backbone_name,\n",
    "            weights='DEFAULT',\n",
    "            trainable_layers=4\n",
    "        )\n",
    "        \n",
    "        # Anchor generator\n",
    "        anchor_sizes = kwargs.get('anchor_sizes', ((32, 64, 128, 256, 512),) * 5)\n",
    "        aspect_ratios = kwargs.get('aspect_ratios', ((0.5, 1.0, 2.0),) * 5)\n",
    "        \n",
    "        # Преобразуем в правильный формат для AnchorGenerator\n",
    "        if isinstance(anchor_sizes[0], int):\n",
    "            anchor_sizes = (tuple(anchor_sizes),) * 5\n",
    "        elif isinstance(anchor_sizes[0], (list, tuple)) and len(anchor_sizes) == 1:\n",
    "            anchor_sizes = (tuple(anchor_sizes[0]),) * 5\n",
    "        \n",
    "        anchor_generator = AnchorGenerator(\n",
    "            sizes=anchor_sizes,\n",
    "            aspect_ratios=aspect_ratios\n",
    "        )\n",
    "        \n",
    "        # Создание модели\n",
    "        model = RetinaNet(\n",
    "            backbone,\n",
    "            num_classes=num_classes,\n",
    "            anchor_generator=anchor_generator,\n",
    "            min_size=kwargs.get('min_size', 800),\n",
    "            max_size=kwargs.get('max_size', 1333),\n",
    "            score_thresh=0.05,\n",
    "            nms_thresh=0.5\n",
    "        )\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    # @staticmethod\n",
    "    # def create_myArch(num_classes: int = 2, **kwargs):\n",
    "    #     \"\"\"Создание своей модели детектора\"\"\"\n",
    "    #     backbone_name = kwargs.get('backbone', 'efficientnet_b0')\n",
    "    #     input_size = kwargs.get('input_size', (640, 640))\n",
    "    #     neck_channels = kwargs.get('neck_channels', 256)\n",
    "    #     anchor_sizes = kwargs.get('anchor_sizes', (32, 64, 128, 256, 512))\n",
    "    #     anchor_ratios = kwargs.get('anchor_ratios', (0.5, 1.0, 2.0))\n",
    "    #     score_thresh = kwargs.get('score_thresh', 0.05)\n",
    "    #     nms_thresh = kwargs.get('nms_thresh', 0.5)\n",
    "        \n",
    "    #     model = MyArchDetector(\n",
    "    #         backbone_name=backbone_name,\n",
    "    #         num_classes=num_classes,\n",
    "    #         anchor_sizes=anchor_sizes,\n",
    "    #         anchor_ratios=anchor_ratios,\n",
    "    #         input_size=input_size,\n",
    "    #         neck_channels=neck_channels,\n",
    "    #         score_thresh=score_thresh,\n",
    "    #         nms_thresh=nms_thresh\n",
    "    #     )\n",
    "       \n",
    "    #     return model\n",
    "    \n",
    "    @staticmethod\n",
    "    def create_myArch(num_classes: int = 2, **kwargs):\n",
    "        \"\"\"Создание максимально простой модели\"\"\"\n",
    "        model = MyArchDetector(\n",
    "            num_classes=num_classes,\n",
    "            input_size=kwargs.get('input_size', (640, 640))\n",
    "        )\n",
    "        return model\n",
    "    \n",
    "# ============================================================================\n",
    "# МЕТРИКИ\n",
    "# ============================================================================\n",
    "\n",
    "class MetricsCalculator:\n",
    "    \"\"\"Калькулятор метрик для детекции\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def calculate_metrics(predictions: List[Dict], targets: List[Dict], \n",
    "                         iou_thresholds: List[float] = [0.5, 0.75]) -> Dict[str, float]:\n",
    "        \"\"\"Вычисляет метрики детекции\"\"\"\n",
    "        if not predictions or not targets:\n",
    "            return {f'ap_{iou}': 0.0 for iou in iou_thresholds}\n",
    "        \n",
    "        metrics = {}\n",
    "        \n",
    "        for iou_thresh in iou_thresholds:\n",
    "            precision, recall, ap = MetricsCalculator._calculate_ap(\n",
    "                predictions, targets, iou_thresh\n",
    "            )\n",
    "            metrics[f'precision_{iou_thresh}'] = precision\n",
    "            metrics[f'recall_{iou_thresh}'] = recall\n",
    "            metrics[f'ap_{iou_thresh}'] = ap\n",
    "        \n",
    "        # mAP\n",
    "        metrics['map'] = np.mean([metrics[f'ap_{iou}'] for iou in iou_thresholds])\n",
    "        \n",
    "        # F1-score для IoU=0.5\n",
    "        p, r = metrics.get('precision_0.5', 0), metrics.get('recall_0.5', 0)\n",
    "        metrics['f1'] = 2 * p * r / (p + r + 1e-10) if (p + r) > 0 else 0.0\n",
    "        \n",
    "        return metrics\n",
    "    \n",
    "    @staticmethod\n",
    "    def _calculate_ap(predictions: List[Dict], targets: List[Dict], \n",
    "                     iou_threshold: float = 0.5) -> Tuple[float, float, float]:\n",
    "        \"\"\"Вычисляет Average Precision для заданного IoU порога\"\"\"\n",
    "        all_pred_boxes, all_pred_scores, all_target_boxes = [], [], []\n",
    "        \n",
    "        # Сбор всех предсказаний и целей\n",
    "        for pred, target in zip(predictions, targets):\n",
    "            if len(pred['boxes']) > 0:\n",
    "                all_pred_boxes.append(pred['boxes'].cpu().numpy())\n",
    "                all_pred_scores.append(pred['scores'].cpu().numpy())\n",
    "            \n",
    "            if len(target['boxes']) > 0:\n",
    "                all_target_boxes.append(target['boxes'].cpu().numpy())\n",
    "        \n",
    "        if not all_pred_boxes or not all_target_boxes:\n",
    "            return 0.0, 0.0, 0.0\n",
    "        \n",
    "        # Объединение\n",
    "        pred_boxes = np.vstack(all_pred_boxes)\n",
    "        pred_scores = np.concatenate(all_pred_scores)\n",
    "        target_boxes = np.vstack(all_target_boxes)\n",
    "        \n",
    "        # Сортировка по уверенности\n",
    "        if len(pred_scores) > 0:\n",
    "            sorted_indices = np.argsort(pred_scores)[::-1]\n",
    "            pred_boxes = pred_boxes[sorted_indices]\n",
    "            pred_scores = pred_scores[sorted_indices]\n",
    "        \n",
    "        # Матрица IoU\n",
    "        iou_matrix = MetricsCalculator._calculate_iou_matrix(pred_boxes, target_boxes)\n",
    "        \n",
    "        # Сопоставление\n",
    "        matches = []\n",
    "        used_targets = set()\n",
    "        \n",
    "        for i in range(len(pred_boxes)):\n",
    "            if iou_matrix.shape[1] == 0:  # Нет целей\n",
    "                matches.append((i, -1, 0))\n",
    "                continue\n",
    "            \n",
    "            ious = iou_matrix[i]\n",
    "            best_iou_idx = np.argmax(ious)\n",
    "            best_iou = ious[best_iou_idx]\n",
    "            \n",
    "            if best_iou >= iou_threshold and best_iou_idx not in used_targets:\n",
    "                matches.append((i, best_iou_idx, 1))\n",
    "                used_targets.add(best_iou_idx)\n",
    "            else:\n",
    "                matches.append((i, -1, 0))\n",
    "        \n",
    "        # Precision и Recall\n",
    "        if matches:\n",
    "            tp_cumsum = np.cumsum([m[2] for m in matches])\n",
    "            fp_cumsum = np.cumsum([1 - m[2] for m in matches])\n",
    "            \n",
    "            precision = tp_cumsum / (tp_cumsum + fp_cumsum + 1e-10)\n",
    "            recall = tp_cumsum / (len(target_boxes) + 1e-10)\n",
    "            \n",
    "            # Average Precision\n",
    "            ap = MetricsCalculator._calculate_average_precision(precision, recall)\n",
    "            \n",
    "            return (precision[-1] if len(precision) > 0 else 0.0,\n",
    "                    recall[-1] if len(recall) > 0 else 0.0,\n",
    "                    ap)\n",
    "        else:\n",
    "            return 0.0, 0.0, 0.0\n",
    "    \n",
    "    @staticmethod\n",
    "    def _calculate_iou_matrix(boxes1: np.ndarray, boxes2: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Вычисляет матрицу IoU между двумя наборами боксов\"\"\"\n",
    "        if len(boxes1) == 0 or len(boxes2) == 0:\n",
    "            return np.zeros((len(boxes1), len(boxes2)))\n",
    "        \n",
    "        boxes1_tensor = torch.tensor(boxes1)\n",
    "        boxes2_tensor = torch.tensor(boxes2)\n",
    "        iou_matrix = box_iou(boxes1_tensor, boxes2_tensor)\n",
    "        \n",
    "        return iou_matrix.numpy()\n",
    "    \n",
    "    @staticmethod\n",
    "    def _calculate_average_precision(precision: np.ndarray, recall: np.ndarray) -> float:\n",
    "        \"\"\"Вычисляет Average Precision по кривой Precision-Recall\"\"\"\n",
    "        if len(precision) == 0 or len(recall) == 0:\n",
    "            return 0.0\n",
    "        \n",
    "        precision = np.concatenate(([0.], precision, [0.]))\n",
    "        recall = np.concatenate(([0.], recall, [1.]))\n",
    "        \n",
    "        # Сглаживание precision\n",
    "        for i in range(len(precision) - 1, 0, -1):\n",
    "            precision[i - 1] = max(precision[i - 1], precision[i])\n",
    "        \n",
    "        # Вычисление AP\n",
    "        indices = np.where(recall[1:] != recall[:-1])[0]\n",
    "        ap = np.sum((recall[indices + 1] - recall[indices]) * precision[indices + 1])\n",
    "        \n",
    "        return ap\n",
    "\n",
    "# ============================================================================\n",
    "# ТРЕНИРОВКА\n",
    "# ============================================================================\n",
    "\n",
    "class Trainer:\n",
    "    \"\"\"Класс для тренировки моделей\"\"\"\n",
    "    \n",
    "    def __init__(self, config: Config, model: nn.Module, \n",
    "                 train_loader: DataLoader, val_loader: DataLoader):\n",
    "        self.config = config\n",
    "        self.model = model\n",
    "        self.train_loader = train_loader\n",
    "        self.val_loader = val_loader\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        \n",
    "        self.model.to(self.device)\n",
    "        \n",
    "        # # Оптимизатор и scheduler\n",
    "        # self.optimizer = optim.AdamW(\n",
    "        #     model.parameters(),\n",
    "        #     lr=config.LEARNING_RATE,\n",
    "        #     weight_decay=config.WEIGHT_DECAY\n",
    "        # )\n",
    "\n",
    "        self.optimizer = optim.SGD(\n",
    "            model.parameters(),\n",
    "            lr=config.LEARNING_RATE,\n",
    "            momentum=0.9,\n",
    "            weight_decay=config.WEIGHT_DECAY,\n",
    "            nesterov=True\n",
    "        )\n",
    "        \n",
    "        self.scheduler = optim.lr_scheduler.MultiStepLR(\n",
    "            self.optimizer,\n",
    "            milestones=[20, 40, 60],\n",
    "            gamma=0.1\n",
    "        )\n",
    "\n",
    "        # scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "        #     optimizer,\n",
    "        #     T_max=num_epochs,\n",
    "        #     eta_min=0.00001\n",
    "        # )\n",
    "        \n",
    "        self.train_losses = []\n",
    "        self.val_metrics_history = []\n",
    "        self.best_map = 0.0\n",
    "    \n",
    "    def train_one_epoch(self, epoch: int) -> float:\n",
    "        \"\"\"Одна эпоха тренировки\"\"\"\n",
    "        self.model.train()\n",
    "        total_loss = 0.0\n",
    "        batch_count = 0\n",
    "        \n",
    "        progress_bar = tqdm(self.train_loader, desc=f\"Эпоха {epoch+1}\")\n",
    "        \n",
    "        for batch_idx, (images, targets) in enumerate(progress_bar):\n",
    "            if len(images) == 0:\n",
    "                continue\n",
    "            \n",
    "            # Перемещение на устройство\n",
    "            images = [img.to(self.device) for img in images]\n",
    "            targets = [{k: v.to(self.device) for k, v in t.items()} for t in targets]\n",
    "            \n",
    "            try:\n",
    "                # Forward pass\n",
    "                loss_dict = self.model(images, targets)\n",
    "                losses = sum(loss for loss in loss_dict.values())\n",
    "                \n",
    "                # Проверка на NaN/Inf\n",
    "                if not torch.isfinite(losses):\n",
    "                    print(f\"   NaN/Inf loss в батче {batch_idx}\")\n",
    "                    self.optimizer.zero_grad()\n",
    "                    continue\n",
    "                \n",
    "                # Backward pass\n",
    "                self.optimizer.zero_grad()\n",
    "                losses.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=10.0)\n",
    "                self.optimizer.step()\n",
    "                \n",
    "                total_loss += losses.item()\n",
    "                batch_count += 1\n",
    "                \n",
    "                # Обновление progress bar\n",
    "                progress_bar.set_postfix({\"loss\": f\"{losses.item():.4f}\"})\n",
    "                #         # Логируем дополнительные метрики\n",
    "                # if 'pos_anchors_avg' in loss_dict:\n",
    "                #     progress_bar.set_postfix({\n",
    "                #         \"loss\": f\"{losses.item():.4f}\",\n",
    "                #         \"pos\": f\"{loss_dict['pos_anchors_avg'].item():.1f}\"\n",
    "                #     })\n",
    "\n",
    "            except RuntimeError as e:\n",
    "                print(f\"   Ошибка в батче {batch_idx}: {e}\")\n",
    "                continue\n",
    "               \n",
    "            \n",
    "        return total_loss / batch_count if batch_count > 0 else float('nan')\n",
    "    \n",
    "    def evaluate(self, num_samples: int = None) -> Dict[str, float]:\n",
    "        \"\"\"Оценка модели\"\"\"\n",
    "        self.model.eval()\n",
    "        all_predictions = []\n",
    "        all_targets = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for i, (images, targets) in enumerate(tqdm(self.val_loader, desc=\"Валидация\")):\n",
    "                if num_samples and i >= num_samples:\n",
    "                    break\n",
    "                \n",
    "                if len(images) == 0:\n",
    "                    continue\n",
    "                \n",
    "                # Перемещение на устройство\n",
    "                images = [img.to(self.device) for img in images]\n",
    "                \n",
    "                # Предсказание\n",
    "                predictions = self.model(images)\n",
    "                \n",
    "                all_predictions.extend(predictions)\n",
    "                all_targets.extend(targets)\n",
    "        \n",
    "        # Вычисление метрик\n",
    "        metrics = MetricsCalculator.calculate_metrics(all_predictions, all_targets)\n",
    "        self.model.train()\n",
    "        \n",
    "        return metrics\n",
    "    \n",
    "    def train(self, num_epochs: int) -> Dict[str, Any]:\n",
    "        \"\"\"Полный цикл тренировки\"\"\"\n",
    "        print(f\"\\nНачало тренировки на {num_epochs} эпох\")\n",
    "        print(f\"Устройство: {self.device}\")\n",
    "        print(f\"Количество тренировочных батчей: {len(self.train_loader)}\")\n",
    "        print(f\"Количество валидационных батчей: {len(self.val_loader)}\")\n",
    "        \n",
    "        patience_counter = 0\n",
    "        \n",
    "        for epoch in range(num_epochs):\n",
    "            print(f\"\\nЭпоха {epoch + 1}/{num_epochs}\")\n",
    "            print(f\"  LR: {self.optimizer.param_groups[0]['lr']:.6f}\")\n",
    "            \n",
    "            # Тренировка\n",
    "            train_loss = self.train_one_epoch(epoch)\n",
    "            self.train_losses.append(train_loss)\n",
    "            \n",
    "            # Валидация\n",
    "            val_metrics = self.evaluate(num_samples=20)\n",
    "            self.val_metrics_history.append(val_metrics)\n",
    "            \n",
    "            # Обновление learning rate\n",
    "            self.scheduler.step()\n",
    "            \n",
    "            # Вывод метрик\n",
    "            print(f\"  Train Loss: {train_loss:.4f}\")\n",
    "            print(f\"  Val mAP: {val_metrics['map']:.4f}\")\n",
    "            print(f\"  AP@0.5: {val_metrics['ap_0.5']:.4f}\")\n",
    "            print(f\"  AP@0.75: {val_metrics['ap_0.75']:.4f}\")\n",
    "            \n",
    "            # Сохранение лучшей модели\n",
    "            if val_metrics['map'] > self.best_map:\n",
    "                self.best_map = val_metrics['map']\n",
    "                self.save_checkpoint(epoch, is_best=True)\n",
    "                patience_counter = 0\n",
    "                print(f\"   Лучшая модель сохранена (mAP: {self.best_map:.4f})\")\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "                print(f\"  Patience: {patience_counter}/{self.config.PATIENCE}\")\n",
    "            \n",
    "            # Ранняя остановка\n",
    "            if patience_counter >= self.config.PATIENCE:\n",
    "                print(f\"  Ранняя остановка на эпохе {epoch + 1}\")\n",
    "                break\n",
    "        \n",
    "        # Сохранение финальной модели\n",
    "        self.save_checkpoint(num_epochs - 1, is_best=False, final=True)\n",
    "        \n",
    "        return {\n",
    "            'train_losses': self.train_losses,\n",
    "            'val_metrics': self.val_metrics_history,\n",
    "            'best_metrics': self.best_map\n",
    "        }\n",
    "    \n",
    "    def save_checkpoint(self, epoch: int, is_best: bool = False, final: bool = False):\n",
    "        \"\"\"Сохранение чекпоинта модели\"\"\"\n",
    "        checkpoint = {\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': self.model.state_dict(),\n",
    "            'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "            'scheduler_state_dict': self.scheduler.state_dict(),\n",
    "            'best_map': self.best_map,\n",
    "            'train_losses': self.train_losses,\n",
    "            'val_metrics': self.val_metrics_history\n",
    "        }\n",
    "        \n",
    "        Path(self.config.BASE_WORKING_DIR).mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        if is_best:\n",
    "            path = Path(self.config.BASE_WORKING_DIR) / 'best_model.pth'\n",
    "        elif final:\n",
    "            path = Path(self.config.BASE_WORKING_DIR) / 'final_model.pth'\n",
    "        else:\n",
    "            path = Path(self.config.BASE_WORKING_DIR) / f'checkpoint_epoch_{epoch}.pth'\n",
    "        \n",
    "        torch.save(checkpoint, path)\n",
    "        print(f\"  Чекпоинт сохранен: {path}\")\n",
    "    \n",
    "    def load_checkpoint(self, checkpoint_path: Path):\n",
    "        \"\"\"Загрузка чекпоинта\"\"\"\n",
    "        checkpoint = torch.load(checkpoint_path, map_location=self.device, weights_only=False)\n",
    "        self.model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        self.scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "        \n",
    "        self.train_losses = checkpoint.get('train_losses', [])\n",
    "        self.val_metrics_history = checkpoint.get('val_metrics', [])\n",
    "        self.best_map = checkpoint.get('best_map', 0.0)\n",
    "        \n",
    "        print(f\"Загружен чекпоинт эпохи {checkpoint['epoch']}\")\n",
    "\n",
    "# ============================================================================\n",
    "# ВИЗУАЛИЗАЦИЯ\n",
    "# ============================================================================\n",
    "\n",
    "class Visualizer:\n",
    "    \"\"\"Класс для визуализации результатов\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def plot_training_curves(train_losses: List[float], val_maps: List[float], \n",
    "                            model_name: str = \"Model\"):\n",
    "        \"\"\"Визуализация кривых обучения\"\"\"\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "        \n",
    "        # Loss кривая\n",
    "        ax1.plot(range(1, len(train_losses) + 1), train_losses, 'b-', linewidth=2)\n",
    "        ax1.set_xlabel('Эпоха')\n",
    "        ax1.set_ylabel('Loss')\n",
    "        ax1.set_title(f'{model_name} - Кривая обучения')\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "        \n",
    "        # mAP кривая\n",
    "        if val_maps:\n",
    "            ax2.plot(range(1, len(val_maps) + 1), val_maps, 'g-', linewidth=2)\n",
    "            ax2.set_xlabel('Эпоха')\n",
    "            ax2.set_ylabel('mAP')\n",
    "            ax2.set_title(f'{model_name} - mAP по эпохам')\n",
    "            ax2.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    @staticmethod\n",
    "    def visualize_predictions(model: nn.Module, dataset: ICDAR2015Dataset, \n",
    "                             config: Config, num_samples: int = 3):\n",
    "        \"\"\"Визуализация предсказаний модели\"\"\"\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        model.to(device)\n",
    "        model.eval()\n",
    "        \n",
    "        for i in range(min(num_samples, len(dataset))):\n",
    "            # Загрузка данных\n",
    "            image, target = dataset[i]\n",
    "            image_input = image.unsqueeze(0).to(device)\n",
    "            \n",
    "            # Предсказание\n",
    "            with torch.no_grad():\n",
    "                prediction = model(image_input)[0]\n",
    "            \n",
    "            # Денормализация изображения\n",
    "            mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)\n",
    "            std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)\n",
    "            image_np = image.cpu() * std + mean\n",
    "            image_np = image_np.permute(1, 2, 0).numpy()\n",
    "            image_np = np.clip(image_np, 0, 1)\n",
    "            \n",
    "            # Создание subplots\n",
    "            fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "            \n",
    "            # Ground Truth\n",
    "            ax1.imshow(image_np)\n",
    "            if len(target['boxes']) > 0:\n",
    "                for box in target['boxes'].cpu().numpy():\n",
    "                    x1, y1, x2, y2 = box\n",
    "                    rect = patches.Rectangle(\n",
    "                        (x1, y1), x2 - x1, y2 - y1,\n",
    "                        linewidth=2, edgecolor='green', facecolor='none', alpha=0.7\n",
    "                    )\n",
    "                    ax1.add_patch(rect)\n",
    "            ax1.set_title(f\"Ground Truth ({len(target['boxes'])} объектов)\")\n",
    "            ax1.axis('off')\n",
    "            \n",
    "            # Predictions\n",
    "            ax2.imshow(image_np)\n",
    "            boxes = prediction['boxes'].cpu().numpy()\n",
    "            scores = prediction['scores'].cpu().numpy()\n",
    "            \n",
    "            # Фильтрация по confidence\n",
    "            confidence_threshold = config.CONFIDENCE_THRESHOLD\n",
    "            valid_indices = scores > confidence_threshold\n",
    "            boxes = boxes[valid_indices]\n",
    "            scores = scores[valid_indices]\n",
    "            \n",
    "            for box, score in zip(boxes, scores):\n",
    "                x1, y1, x2, y2 = box\n",
    "                color = (score, 1 - score, 0) if score > 0.5 else (1, score, 0)\n",
    "                rect = patches.Rectangle(\n",
    "                    (x1, y1), x2 - x1, y2 - y1,\n",
    "                    linewidth=2, edgecolor=color, facecolor='none', alpha=0.7\n",
    "                )\n",
    "                ax2.add_patch(rect)\n",
    "                \n",
    "                if score > 0.7:\n",
    "                    ax2.text(x1, y1 - 5, f\"{score:.2f}\", \n",
    "                            color=color, fontsize=8, fontweight='bold')\n",
    "            \n",
    "            ax2.set_title(f\"Предсказания ({len(boxes)} объектов)\")\n",
    "            ax2.axis('off')\n",
    "            \n",
    "            plt.suptitle(f\"Пример {i + 1}\", fontsize=14)\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            \n",
    "            print(f\"Пример {i + 1}: {len(boxes)} обнаружено, {len(target['boxes'])} GT\")\n",
    "            if len(scores) > 0:\n",
    "                print(f\"  Max confidence: {scores.max():.3f}, Mean: {scores.mean():.3f}\")\n",
    "\n",
    "# ============================================================================\n",
    "# ВСПОМОГАТЕЛЬНЫЕ ФУНКЦИИ\n",
    "# ============================================================================\n",
    "\n",
    "def create_dataloaders(config: Config, batch_size: int = None):\n",
    "    \"\"\"Создание DataLoader'ов для обучения и валидации\"\"\"\n",
    "    if batch_size is None:\n",
    "        batch_size = config.BATCH_SIZE\n",
    "    \n",
    "    processed_paths = config.get_processed_paths()\n",
    "    train_paths = config.get_train_paths()\n",
    "    test_paths = config.get_test_paths()\n",
    "    \n",
    "    # Проверка существования файлов\n",
    "    if not processed_paths['train_annotations'].exists():\n",
    "        print(f\"Не найден файл аннотаций: {processed_paths['train_annotations']}\")\n",
    "        return None, None\n",
    "    \n",
    "    # Создание датасетов\n",
    "    train_dataset = ICDAR2015Dataset(\n",
    "        train_paths['images'],\n",
    "        processed_paths['train_annotations'],\n",
    "        transform=get_train_transforms()\n",
    "    )\n",
    "    \n",
    "    val_dataset = ICDAR2015Dataset(\n",
    "        test_paths['images'],\n",
    "        processed_paths['val_annotations'],\n",
    "        transform=get_val_transforms()\n",
    "    )\n",
    "    \n",
    "    print(f\"Тренировочный датасет: {len(train_dataset)} изображений\")\n",
    "    print(f\"Валидационный датасет: {len(val_dataset)} изображений\")\n",
    "    \n",
    "    # Создание DataLoader'ов\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=2,\n",
    "        collate_fn=lambda x: tuple(zip(*x)),\n",
    "        pin_memory=True if torch.cuda.is_available() else False\n",
    "    )\n",
    "    \n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=2,\n",
    "        collate_fn=lambda x: tuple(zip(*x)),\n",
    "        pin_memory=True if torch.cuda.is_available() else False\n",
    "    )\n",
    "    \n",
    "    return train_loader, val_loader\n",
    "\n",
    "def prepare_data(config: Config) -> bool:\n",
    "    \"\"\"Подготовка данных ICDAR2015\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"ПОДГОТОВКА ДАННЫХ ICDAR2015\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Создание директорий\n",
    "    config.ICDAR2015_PROCESSED.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Проверка существования данных\n",
    "    if not config.check_data_structure():\n",
    "        print(\"\\n Не все необходимые данные найдены!\")\n",
    "        print(\"\\nОжидаемая структура:\")\n",
    "        print(f\"{config.ICDAR2015_ROOT}/\")\n",
    "        print(\"  ├── ch4_training_images/\")\n",
    "        print(\"  ├── ch4_training_localization_transcription_gt/\")\n",
    "        print(\"  ├── ch4_test_images/\")\n",
    "        print(\"  └── ch4_test_localization_transcription_gt/\")\n",
    "        return False\n",
    "    \n",
    "    # Получение путей\n",
    "    train_paths = config.get_train_paths()\n",
    "    test_paths = config.get_test_paths()\n",
    "    processed_paths = config.get_processed_paths()\n",
    "    \n",
    "    # Конвертация тренировочных данных\n",
    "    print(\"\\nКонвертация тренировочных данных...\")\n",
    "    converter_train = ICDAR2015ToCOCOConverter(\n",
    "        train_paths['images'],\n",
    "        train_paths['labels'],\n",
    "        [\"text\"]\n",
    "    )\n",
    "    coco_train = converter_train.convert(processed_paths['train_annotations'])\n",
    "    \n",
    "    # Конвертация тестовых данных\n",
    "    print(\"\\nКонвертация тестовых данных...\")\n",
    "    converter_val = ICDAR2015ToCOCOConverter(\n",
    "        test_paths['images'],\n",
    "        test_paths['labels'],\n",
    "        [\"text\"]\n",
    "    )\n",
    "    coco_val = converter_val.convert(processed_paths['val_annotations'])\n",
    "    \n",
    "    # Сохранение конфигурации\n",
    "    config_data = {\n",
    "        \"platform\": config.platform,\n",
    "        \"train\": {\n",
    "            \"images\": str(train_paths['images']),\n",
    "            \"annotations\": str(processed_paths['train_annotations'])\n",
    "        },\n",
    "        \"val\": {\n",
    "            \"images\": str(test_paths['images']),\n",
    "            \"annotations\": str(processed_paths['val_annotations'])\n",
    "        },\n",
    "        \"categories\": [\"text\"],\n",
    "        \"num_classes\": 2,\n",
    "        \"processed_dir\": str(config.ICDAR2015_PROCESSED)\n",
    "    }\n",
    "    \n",
    "    with open(processed_paths['config'], 'w') as f:\n",
    "        json.dump(config_data, f, indent=2)\n",
    "    \n",
    "    print(f\"\\n Конфигурация сохранена: {processed_paths['config']}\")\n",
    "    print(\"\\nСВОДКА ДАННЫХ:\")\n",
    "    print(f\"  Тренировочные: {len(coco_train['images'])} изображений, {len(coco_train['annotations'])} аннотаций\")\n",
    "    print(f\"  Тестовые: {len(coco_val['images'])} изображений, {len(coco_val['annotations'])} аннотаций\")\n",
    "    \n",
    "    return True\n",
    "\n",
    "def run_training(config: Config, architecture: str, **kwargs):\n",
    "    \"\"\"Запуск обучения модели\"\"\"\n",
    "    \n",
    "    print(f\"\\nОбучение модели {architecture}...\")\n",
    "\n",
    "    if architecture == 'myArch':\n",
    "        kwargs.update({\n",
    "            'backbone': config.MYARCH_BACKBONE,\n",
    "            'input_size': config.MYARCH_INPUT_SIZE,\n",
    "            'neck_channels': config.MYARCH_NECK_CHANNELS,\n",
    "            'anchor_sizes': config.MYARCH_ANCHOR_SIZES,\n",
    "            'anchor_ratios': config.MYARCH_ANCHOR_RATIOS\n",
    "        })\n",
    "    \n",
    "    # Создание модели\n",
    "    model = ModelFactory.create_model(\n",
    "        architecture=architecture,\n",
    "        num_classes=config.NUM_CLASSES,\n",
    "        **kwargs\n",
    "    )\n",
    "    \n",
    "    # Создание DataLoader'ов\n",
    "    train_loader, val_loader = create_dataloaders(config)\n",
    "    \n",
    "    if train_loader is None or val_loader is None:\n",
    "        print(\"Не удалось создать DataLoader'ы\")\n",
    "        return None\n",
    "    \n",
    "    # Создание тренера\n",
    "    trainer = Trainer(config, model, train_loader, val_loader)\n",
    "    \n",
    "    # Запуск обучения\n",
    "    results = trainer.train(config.NUM_EPOCHS)\n",
    "    \n",
    "    # Визуализация результатов\n",
    "    if results['val_metrics']:\n",
    "        Visualizer.plot_training_curves(\n",
    "            results['train_losses'],\n",
    "            [m['map'] for m in results['val_metrics']],\n",
    "            model_name=architecture\n",
    "        )\n",
    "    \n",
    "    return results\n",
    "\n",
    "def test_model(config: Config, model_path: str, architecture: str = None):\n",
    "    \"\"\"Тестирование модели\"\"\"\n",
    "    \n",
    "    print(f\"\\nТестирование модели из {model_path}\")\n",
    "    \n",
    "    # Определение архитектуры\n",
    "    if architecture is None:\n",
    "        if 'faster_rcnn' in model_path.lower():\n",
    "            architecture = 'faster_rcnn'\n",
    "        elif 'retinanet' in model_path.lower():\n",
    "            architecture = 'retinanet'\n",
    "        elif 'myArch' in model_path.lower():\n",
    "            architecture = 'myArch'\n",
    "        else:\n",
    "            architecture = 'faster_rcnn'\n",
    "    \n",
    "    # Загрузка модели\n",
    "    model = ModelFactory.create_model(\n",
    "        architecture=architecture,\n",
    "        num_classes=config.NUM_CLASSES\n",
    "    )\n",
    "    \n",
    "    checkpoint = torch.load(model_path, map_location='cpu')\n",
    "    if 'model_state_dict' in checkpoint:\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    else:\n",
    "        model.load_state_dict(checkpoint)\n",
    "    \n",
    "    # Создание DataLoader для тестирования\n",
    "    _, val_loader = create_dataloaders(config, batch_size=1)\n",
    "    if val_loader is None:\n",
    "        print(\"Не удалось создать DataLoader для тестирования\")\n",
    "        return None\n",
    "    \n",
    "    val_dataset = val_loader.dataset\n",
    "    \n",
    "    # Визуализация предсказаний\n",
    "    Visualizer.visualize_predictions(model, val_dataset, config, num_samples=config.VISUALIZE_SAMPLES)\n",
    "    \n",
    "    # Оценка модели\n",
    "    print(\"\\nОЦЕНКА МОДЕЛИ НА ВАЛИДАЦИОННОМ НАБОРЕ...\")\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    all_predictions = []\n",
    "    all_targets = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, targets in tqdm(val_loader, desc=\"Тестирование\"):\n",
    "            if len(images) == 0:\n",
    "                continue\n",
    "            \n",
    "            images = [img.to(device) for img in images]\n",
    "            predictions = model(images)\n",
    "            \n",
    "            all_predictions.extend(predictions)\n",
    "            all_targets.extend(targets)\n",
    "    \n",
    "    # Вычисление метрик\n",
    "    metrics = MetricsCalculator.calculate_metrics(all_predictions, all_targets)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"РЕЗУЛЬТАТЫ ТЕСТИРОВАНИЯ\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"mAP: {metrics['map']:.4f}\")\n",
    "    print(f\"AP@0.5: {metrics['ap_0.5']:.4f}\")\n",
    "    print(f\"AP@0.75: {metrics['ap_0.75']:.4f}\")\n",
    "    print(f\"F1-Score: {metrics['f1']:.4f}\")\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "# ============================================================================\n",
    "# ГЛАВНАЯ ФУНКЦИЯ\n",
    "# ============================================================================\n",
    "\n",
    "def main():\n",
    "    \"\"\"Главная функция, управляемая конфигом\"\"\"\n",
    "    \n",
    "    # Инициализация конфигурации\n",
    "    config = Config()\n",
    "    config.print_info()\n",
    "    \n",
    "    # Проверка данных\n",
    "    if not config.ICDAR2015_ROOT.exists():\n",
    "        print(f\"\\nДанные не найдены по пути: {config.ICDAR2015_ROOT}\")\n",
    "        print(\"\\nИНСТРУКЦИЯ ПО ПОДГОТОВКЕ ДАННЫХ:\")\n",
    "        print(\"1. Скачайте ICDAR2015 с https://rrc.cvc.uab.es/?ch=4\")\n",
    "        print(\"2. Распакуйте в следующую структуру:\")\n",
    "        print(f\"   {config.ICDAR2015_ROOT}/\")\n",
    "        print(\"   ├── ch4_training_images/\")\n",
    "        print(\"   ├── ch4_training_localization_transcription_gt/\")\n",
    "        print(\"   ├── ch4_test_images/\")\n",
    "        print(\"   └── ch4_test_localization_transcription_gt/\")\n",
    "        print(\"\\n3. Перезапустите скрипт\")\n",
    "        return\n",
    "    \n",
    "    # Подготовка данных\n",
    "    processed_paths = config.get_processed_paths()\n",
    "    if not processed_paths['config'].exists():\n",
    "        print(f\"\\n Обработанные данные не найдены\")\n",
    "        success = prepare_data(config)\n",
    "        if not success:\n",
    "            return\n",
    "    \n",
    "    # Выполнение в зависимости от режима\n",
    "    if config.MODE == 'train':\n",
    "        print(f\"\\nРежим: обучение модели {config.ARCHITECTURE}\")\n",
    "        params = {\n",
    "            'min_size': config.MIN_SIZE,\n",
    "            'max_size': config.MAX_SIZE,\n",
    "            'backbone': config.BACKBONE\n",
    "        }\n",
    "        results = run_training(config, config.ARCHITECTURE, **params)\n",
    "        \n",
    "        if results:\n",
    "            # Сохранение результатов\n",
    "            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "            results_file = Path(config.BASE_WORKING_DIR) / f\"results_{config.ARCHITECTURE}_{timestamp}.json\"\n",
    "            \n",
    "            with open(results_file, 'w') as f:\n",
    "                json.dump({\n",
    "                    'architecture': config.ARCHITECTURE,\n",
    "                    'config': {\n",
    "                        'num_epochs': config.NUM_EPOCHS,\n",
    "                        'batch_size': config.BATCH_SIZE,\n",
    "                        'learning_rate': config.LEARNING_RATE\n",
    "                    },\n",
    "                    'results': {\n",
    "                        'best_map': results['best_metrics'],\n",
    "                        'final_map': results['val_metrics'][-1]['map'] if results['val_metrics'] else 0\n",
    "                    }\n",
    "                }, f, indent=2)\n",
    "            \n",
    "            print(f\"\\n Результаты сохранены: {results_file}\")\n",
    "    \n",
    "    elif config.MODE == 'test':\n",
    "        print(f\"\\nРежим: тестирование модели\")\n",
    "        # Поиск модели, если не указан путь\n",
    "        if config.MODEL_PATH is None:\n",
    "            model_files = list(Path(config.BASE_WORKING_DIR).glob(\"*.pth\"))\n",
    "            if not model_files:\n",
    "                print(\"Сохраненных моделей не найдено!\")\n",
    "                return\n",
    "            # Берем последнюю модель\n",
    "            config.MODEL_PATH = str(model_files[-1])\n",
    "            print(f\"Используется модель: {config.MODEL_PATH}\")\n",
    "        \n",
    "        test_model(config, config.MODEL_PATH, config.ARCHITECTURE)\n",
    "    \n",
    "    elif config.MODE == 'compare':\n",
    "        print(f\"\\nРежим: сравнение архитектур\")\n",
    "        architectures = config.COMPARE_ARCHITECTURES\n",
    "        results = {}\n",
    "        for arch in architectures:\n",
    "            print(f\"\\nОбучение {arch}...\")\n",
    "            try:\n",
    "                result = run_training(\n",
    "                    config,\n",
    "                    arch,\n",
    "                    num_epochs=min(10, config.NUM_EPOCHS)\n",
    "                )\n",
    "                if result:\n",
    "                    results[arch] = result['best_metrics']\n",
    "                    print(f\" {arch}: mAP = {result['best_metrics']:.4f}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Ошибка при обучении {arch}: {e}\")\n",
    "        \n",
    "        # Визуализация сравнения\n",
    "        if results:\n",
    "            print(\"\\nРЕЗУЛЬТАТЫ СРАВНЕНИЯ:\")\n",
    "            for arch, score in sorted(results.items(), key=lambda x: x[1], reverse=True):\n",
    "                print(f\"  {arch}: {score:.4f}\")\n",
    "    \n",
    "    elif config.MODE == 'visualize':\n",
    "        print(f\"\\\\nРежим: визуализация предсказаний\")\n",
    "        # Создание датасета\n",
    "        processed_paths = config.get_processed_paths()\n",
    "        test_paths = config.get_test_paths()\n",
    "        \n",
    "        dataset = ICDAR2015Dataset(\n",
    "            test_paths['images'],\n",
    "            processed_paths['val_annotations'],\n",
    "            transform=get_val_transforms(),\n",
    "            max_size=config.VISUALIZE_SAMPLES\n",
    "        )\n",
    "        \n",
    "        # Загрузка модели\n",
    "        if config.MODEL_PATH is None:\n",
    "            model_files = list(Path(config.BASE_WORKING_DIR).glob(\"*.pth\"))\n",
    "            if not model_files:\n",
    "                print(\"Сохраненных моделей не найдено!\")\n",
    "                return\n",
    "            config.MODEL_PATH = str(model_files[-1])\n",
    "        \n",
    "        print(f\"Используется модель: {config.MODEL_PATH}\")\n",
    "        \n",
    "        # Определение архитектуры по имени файла, если не задано\n",
    "        if config.ARCHITECTURE is None:\n",
    "            if 'faster_rcnn' in config.MODEL_PATH.lower():\n",
    "                config.ARCHITECTURE = 'faster_rcnn'\n",
    "            elif 'retinanet' in config.MODEL_PATH.lower():\n",
    "                config.ARCHITECTURE = 'retinanet'\n",
    "            else:\n",
    "                config.ARCHITECTURE = 'faster_rcnn'\n",
    "        \n",
    "        # Загрузка модели\n",
    "        model = ModelFactory.create_model(\n",
    "            architecture=config.ARCHITECTURE,\n",
    "            num_classes=config.NUM_CLASSES\n",
    "        )\n",
    "        \n",
    "        checkpoint = torch.load(config.MODEL_PATH, map_location='cpu', weights_only=False)\n",
    "        if 'model_state_dict' in checkpoint:\n",
    "            model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        else:\n",
    "            model.load_state_dict(checkpoint)\n",
    "        \n",
    "        # Перемещение модели на правильное устройство\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        model.to(device)  \n",
    "        model.eval()\n",
    "        \n",
    "        # Визуализация\n",
    "        Visualizer.visualize_predictions(model, dataset, config, num_samples=config.VISUALIZE_SAMPLES)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-08T10:58:39.845166Z",
     "iopub.status.busy": "2026-02-08T10:58:39.844635Z",
     "iopub.status.idle": "2026-02-08T10:58:43.492862Z",
     "shell.execute_reply": "2026-02-08T10:58:43.492238Z",
     "shell.execute_reply.started": "2026-02-08T10:58:39.845140Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# ТОЧКА ВХОДА\n",
    "# ============================================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 1150310,
     "sourceId": 1928836,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31259,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
